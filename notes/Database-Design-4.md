## Part 1: Database Roles and Access Control

### What are Database Roles?

**Definition:** A database role is an entity that contains information about:
1. **Privileges:** What the role can do (login, create databases, etc.)
2. **Authentication:** How the role interacts with the client (passwords, etc.)

**Key characteristics:**
- Roles are global across all databases in your cluster
- Roles can be assigned to one or more users
- Roles are separate from operating system users

---

### Creating Roles

#### Basic Syntax
```sql
CREATE ROLE role_name;
```

#### Examples

**1. Create a data analyst role:**
```sql
CREATE ROLE data_analyst;
```
*Currently empty - no privileges defined yet*

**2. Create an intern role with expiration:**
```sql
CREATE ROLE intern 
WITH PASSWORD 'secure_password' 
VALID UNTIL '2020-01-01';
```
*One second into 2020, the password becomes invalid*

**3. Create an admin role with database creation privilege:**
```sql
CREATE ROLE admin WITH CREATEDB;
```

---

### Modifying Roles

#### ALTER Syntax
```sql
ALTER ROLE role_name WITH attribute;
```

**Example - Add ability to create roles:**
```sql
ALTER ROLE admin WITH CREATEROLE;
```

---

### Common Role Attributes

| Attribute | Description |
|-----------|-------------|
| `LOGIN` | Allows role to log in |
| `SUPERUSER` | Has all privileges |
| `CREATEDB` | Can create databases |
| `CREATEROLE` | Can create other roles |
| `PASSWORD 'text'` | Sets password |
| `VALID UNTIL 'timestamp'` | Password expiration date |

---

### GRANT and REVOKE Privileges

#### Syntax
```sql
-- Grant privileges
GRANT privilege_type ON object_name TO role_name;

-- Revoke privileges
REVOKE privilege_type ON object_name FROM role_name;
```

#### Common Privileges (PostgreSQL)

| Privilege | Description |
|-----------|-------------|
| `SELECT` | Read data from table/view |
| `INSERT` | Add new rows |
| `UPDATE` | Modify existing data |
| `DELETE` | Remove rows |
| `TRUNCATE` | Empty table |
| `REFERENCES` | Create foreign keys |
| `TRIGGER` | Create triggers |
| `CREATE` | Create objects in schema |
| `CONNECT` | Connect to database |
| `TEMPORARY` | Create temporary tables |
| `EXECUTE` | Execute functions |
| `USAGE` | Use schema/sequences |

#### Examples

**Grant UPDATE privilege:**
```sql
GRANT UPDATE ON ratings TO data_analyst;
```

**Revoke UPDATE privilege:**
```sql
REVOKE UPDATE ON ratings FROM data_analyst;
```

---

### Users vs Groups (Both are Roles!)

**Important concept:** The role system encompasses both "users" and "groups"

#### User Roles
- Individual accounts
- Typically for one specific person
- Example: `alex`, `intern`

#### Group Roles
- Collections of privileges
- Multiple users can belong to a group
- Example: `data_analyst`, `admin`

#### Visual Representation
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   data_analyst (GROUP)  ‚îÇ
‚îÇ                         ‚îÇ
‚îÇ  - SELECT on tables     ‚îÇ
‚îÇ  - UPDATE on ratings    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îú‚îÄ‚îÄ> alex (USER)
         ‚îÇ
         ‚îú‚îÄ‚îÄ> sarah (USER)
         ‚îÇ
         ‚îî‚îÄ‚îÄ> john (USER)
```

---

### Adding Users to Groups

#### GRANT for Group Membership
```sql
GRANT group_role TO user_role;
```

**Example - Add Alex to data_analyst group:**
```sql
-- First, create the user
CREATE ROLE alex WITH LOGIN PASSWORD 'alex_password';

-- Then, add to group
GRANT data_analyst TO alex;
```

Now Alex has all privileges that `data_analyst` has!

#### REVOKE Group Membership
```sql
REVOKE group_role FROM user_role;
```

**Example:**
```sql
REVOKE data_analyst FROM alex;
```
Alex no longer has data_analyst privileges.

---

### Real-World Example

**Scenario:** Hiring data analysts and an intern

**Step 1: Create group role**
```sql
CREATE ROLE data_analyst;

-- Grant privileges to the group
GRANT SELECT ON ALL TABLES IN SCHEMA public TO data_analyst;
GRANT UPDATE ON ratings TO data_analyst;
GRANT INSERT ON user_feedback TO data_analyst;
```

**Step 2: Create user roles**
```sql
-- Create intern (temporary)
CREATE ROLE intern WITH LOGIN PASSWORD 'intern123' VALID UNTIL '2025-12-31';

-- Add intern to data_analyst group
GRANT data_analyst TO intern;

-- Create permanent analyst
CREATE ROLE sarah WITH LOGIN PASSWORD 'sarah_secure_pw';
GRANT data_analyst TO sarah;

-- Create admin
CREATE ROLE admin WITH LOGIN PASSWORD 'admin_pw' CREATEDB CREATEROLE;
```

**Step 3: When intern leaves**
```sql
REVOKE data_analyst FROM intern;
DROP ROLE intern;
```

**Step 4: When new analyst joins**
```sql
CREATE ROLE john WITH LOGIN PASSWORD 'john_pw';
GRANT data_analyst TO john;
```
*No need to reassign individual privileges!*

---

### Benefits of Roles

| Benefit | Description |
|---------|-------------|
| ‚úÖ **Persistence** | Roles live on even when employees leave |
| ‚úÖ **Efficiency** | Group common access levels, save time |
| ‚úÖ **Pre-creation** | Create roles before employees get accounts |
| ‚úÖ **Consistency** | Everyone in same role has same access |
| ‚úÖ **Easy management** | Add/remove users from groups easily |

### ‚ö†Ô∏è Common Pitfalls

| Pitfall | Solution |
|---------|----------|
| Role gives too much access | Regularly audit role privileges |
| Forgotten to remove users | Document role memberships |
| Overlapping permissions | Use clear naming conventions |

---

## Part 2: Table Partitioning

### Why Partition?

**Problem:** When tables grow to hundreds of gigabytes or terabytes:
- Queries become slow
- Indices become too large to fit in memory
- Database performance degrades

**Solution:** Split tables into smaller parts = **Partitioning**

---

### Data Modeling Context

**Partitioning fits into the Physical Data Model:**
- **Logical:** Data structure remains the same
- **Physical:** Data is distributed over several physical entities

```
Conceptual Model (ER Diagram)
         ‚Üì
Logical Model (Tables, Columns)
         ‚Üì
Physical Model (Partitions, Storage) ‚Üê Partitioning happens here
```

---

### Types of Partitioning

## 1. Vertical Partitioning

**Definition:** Split table by columns (even when normalized)

### Example: Product Table

**Original Table (products):**
| product_id (PK) | product_name | price | long_description |
|-----------------|--------------|-------|------------------|
| 1               | Laptop       | 999   | [5000 chars...] |
| 2               | Mouse        | 25    | [4800 chars...] |
| 3               | Keyboard     | 75    | [4500 chars...] |

**After Vertical Partitioning:**

**products_main** (frequently accessed, fast storage)
| product_id (PK) | product_name | price |
|-----------------|--------------|-------|
| 1               | Laptop       | 999   |
| 2               | Mouse        | 25    |
| 3               | Keyboard     | 75    |

**products_details** (rarely accessed, slower storage)
| product_id (PK) | long_description |
|-----------------|------------------|
| 1               | [5000 chars...] |
| 2               | [4800 chars...] |
| 3               | [4500 chars...] |

**Benefits:**
- ‚úÖ Faster queries on main table (less data to scan)
- ‚úÖ Can store rarely-used data on slower, cheaper storage
- ‚úÖ Reduces I/O for common queries

---

## 2. Horizontal Partitioning

**Definition:** Split table by rows (typically by date/timestamp)

### Example: Book Sales

**Original Table (book_sales):**
| sale_id | book_id | store_id | timestamp  | sales_amount | quantity |
|---------|---------|----------|------------|--------------|----------|
| 1       | 101     | 5        | 2018-01-15 | 250.00       | 10       |
| 2       | 205     | 3        | 2018-04-20 | 180.00       | 6        |
| 3       | 310     | 7        | 2018-07-10 | 125.00       | 5        |
| 4       | 101     | 5        | 2018-10-05 | 300.00       | 12       |
| 5       | 205     | 8        | 2019-01-20 | 200.00       | 8        |
| 6       | 310     | 3        | 2019-04-15 | 150.00       | 6        |

**After Horizontal Partitioning (by Quarter):**

**book_sales_2018_q1**
| sale_id | book_id | store_id | timestamp  | sales_amount | quantity |
|---------|---------|----------|------------|--------------|----------|
| 1       | 101     | 5        | 2018-01-15 | 250.00       | 10       |

**book_sales_2018_q2**
| sale_id | book_id | store_id | timestamp  | sales_amount | quantity |
|---------|---------|----------|------------|--------------|----------|
| 2       | 205     | 3        | 2018-04-20 | 180.00       | 6        |

**book_sales_2018_q3**
| sale_id | book_id | store_id | timestamp  | sales_amount | quantity |
|---------|---------|----------|------------|--------------|----------|
| 3       | 310     | 7        | 2018-07-10 | 125.00       | 5        |

**book_sales_2018_q4**
| sale_id | book_id | store_id | timestamp  | sales_amount | quantity |
|---------|---------|----------|------------|--------------|----------|
| 4       | 101     | 5        | 2018-10-05 | 300.00       | 12       |

---

### Creating Horizontal Partitions (PostgreSQL 10+)

#### Step 1: Create Parent Table
```sql
CREATE TABLE book_sales (
    sale_id INT,
    book_id INT,
    store_id INT,
    timestamp DATE,
    sales_amount DECIMAL,
    quantity INT
) PARTITION BY RANGE (timestamp);
```

#### Step 2: Create Partitions
```sql
-- Q1 2018 partition
CREATE TABLE book_sales_2018_q1 
PARTITION OF book_sales
FOR VALUES FROM ('2018-01-01') TO ('2018-04-01');

-- Q2 2018 partition
CREATE TABLE book_sales_2018_q2 
PARTITION OF book_sales
FOR VALUES FROM ('2018-04-01') TO ('2018-07-01');

-- Q3 2018 partition
CREATE TABLE book_sales_2018_q3 
PARTITION OF book_sales
FOR VALUES FROM ('2018-07-01') TO ('2018-10-01');

-- Q4 2018 partition
CREATE TABLE book_sales_2018_q4 
PARTITION OF book_sales
FOR VALUES FROM ('2018-10-01') TO ('2019-01-01');
```

#### Step 3: Create Index on Partition Column
```sql
CREATE INDEX ON book_sales (timestamp);
```

#### Querying Partitioned Tables
```sql
-- Query specific quarter (only scans one partition)
SELECT * FROM book_sales 
WHERE timestamp BETWEEN '2018-01-01' AND '2018-03-31';

-- Query entire table (scans all partitions)
SELECT * FROM book_sales;
```

---

### Pros and Cons of Horizontal Partitioning

#### ‚úÖ Advantages

| Benefit | Description | Example |
|---------|-------------|---------|
| **Optimized indices** | Indices for each partition are smaller | Index for Q1 2018 fits in memory |
| **Faster queries** | Only relevant partitions are scanned | Query for Jan 2018 only scans Q1 partition |
| **Storage flexibility** | Move old data to slower storage | 2018 data on cheaper HDD, 2024 on SSD |
| **Easier maintenance** | Drop entire partitions | Drop all 2015 data at once |
| **Better for OLAP & OLTP** | Both benefit from partitioning | Analytics on recent data is faster |

#### ‚ùå Disadvantages

| Challenge | Description |
|-----------|-------------|
| **Migration complexity** | Must create new table and copy data |
| **Constraint limitations** | PRIMARY KEY constraints may not work |
| **Management overhead** | Must create and maintain multiple tables |
| **Query complexity** | Some queries need to scan all partitions |

---

### Sharding: Partitioning Across Machines

**Sharding = Horizontal partitioning across multiple servers**

**Example:**
```
Server 1: book_sales_2018_q1, book_sales_2018_q2
Server 2: book_sales_2018_q3, book_sales_2018_q4
Server 3: book_sales_2019_q1, book_sales_2019_q2
```

**Benefits:**
- Massively Parallel Processing (MPP)
- Each server processes its own data
- Scales horizontally

**Use cases:**
- Very large databases (petabytes)
- High-traffic applications
- Distributed systems

---

## Part 3: Data Integration

### What is Data Integration?

**Definition:** Combining data from different sources, formats, and technologies to provide a unified view.

```
Multiple Data Sources ‚Üí Transformations ‚Üí Unified Data Model
```

---

### Business Case Examples

#### 1. 360-Degree Customer View
**Scenario:** View all customer information from different departments

```
Sales DB (PostgreSQL)
    ‚Üì
Marketing DB (MySQL)      ‚Üí  Unified Customer View
    ‚Üì
Support DB (MongoDB)
```

#### 2. Company Acquisition
**Challenge:** Merge databases from two companies

```
Company A Database  ‚Üí
                      ‚Üí  Combined Database
Company B Database  ‚Üí
```

#### 3. Legacy Systems
**Scenario:** Insurance company with old and new claims systems

```
Legacy System (Old claims 1990-2010)
    ‚Üì
Modern System (New claims 2010-2024)  ‚Üí  Unified Claims Database
```

---

### Data Integration Planning

#### 1. Define Your Goal

**Unified data model purposes:**
- üìä **Dashboards:** Daily sales graphs, KPI tracking
- ü§ñ **Data Products:** Recommendation engines, ML models
- üìà **Analytics:** Business intelligence, reporting

**Performance requirement:** Model must be fast enough for use case

---

#### 2. Identify Data Sources

**Example: DataCamp Skill Assessment Launch**

**Goal:** Target customers for new product

**Data needed:**
```
Sales Data (PostgreSQL)
    ‚Üì
    ‚îî‚îÄ> Which customers can afford new product?

Product Data (MongoDB)
    ‚Üì
    ‚îî‚îÄ> Who are potential early adopters?

         ‚Üì
    
Unified Model (Redshift)
    ‚Üì
Marketing Dashboard
```

---

#### 3. Determine Formats

**Data Source Formats:**
- PostgreSQL (relational)
- MongoDB (document store)
- CSV files
- APIs (JSON)
- Excel spreadsheets

**Unified Model Format:**
- Redshift (AWS data warehouse)
- Snowflake (cloud data platform)
- BigQuery (Google)
- Azure SQL Data Warehouse

---

#### 4. Update Cadence

**How often should data refresh?**

| Use Case | Update Frequency | Example |
|----------|-----------------|---------|
| **Sales reports** | Daily | `UPDATE DAILY at 2:00 AM` |
| **Air traffic control** | Real-time | `STREAM continuously` |
| **Customer analytics** | Hourly | `UPDATE HOURLY` |
| **Historical analysis** | Weekly/Monthly | `UPDATE WEEKLY on Sunday` |

‚ö†Ô∏è **Different sources can have different cadences!**

```
Sales DB ‚Üí Daily updates
Product usage ‚Üí Real-time  ‚Üí Unified Model
Customer profiles ‚Üí Weekly
```

---

### Transformations

**Problem:** Sources are in different formats - can't just plug them together!

**Solution:** Transformations extract and convert data to unified format

#### Example Transformation

**Source 1 (PostgreSQL):**
```
customers table:
| id | first_name | last_name | purchase_date |
```

**Source 2 (MongoDB):**
```
{
  "user_id": "123",
  "name": "John Doe",
  "bought_on": "2024-10-15"
}
```

**Unified Model:**
```
| customer_id | full_name | purchase_date |
```

**Transformation:**
```python
# PostgreSQL transformation
customer_id = id
full_name = first_name + " " + last_name
purchase_date = purchase_date

# MongoDB transformation
customer_id = user_id
full_name = name
purchase_date = bought_on
```

---

### Data Integration Tools

#### Hand-Coded Transformations
**Pros:**
- ‚úÖ Full control
- ‚úÖ Custom logic

**Cons:**
- ‚ùå Must create transformation for each source
- ‚ùå High maintenance burden
- ‚ùå Difficult to scale

#### Data Integration Tools (ETL)

**Popular Tools:**
- **Apache Airflow:** Workflow orchestration
- **Scriptella:** ETL framework
- **Talend:** Data integration platform
- **Apache NiFi:** Data flow automation
- **dbt:** Transform data in warehouse

**Example: Apache Airflow DAG**
```python
from airflow import DAG
from airflow.operators.python import PythonOperator

dag = DAG('data_integration', schedule_interval='@daily')

extract_sales = PythonOperator(
    task_id='extract_sales',
    python_callable=extract_sales_data
)

extract_product = PythonOperator(
    task_id='extract_product',
    python_callable=extract_product_data
)

transform = PythonOperator(
    task_id='transform',
    python_callable=transform_data
)

load = PythonOperator(
    task_id='load',
    python_callable=load_to_warehouse
)

[extract_sales, extract_product] >> transform >> load
```

---

### Choosing a Data Integration Tool

#### Key Criteria

| Criterion | Why Important | Example |
|-----------|---------------|---------|
| **Flexibility** | Connects to all your sources | PostgreSQL, MongoDB, APIs, CSV |
| **Reliability** | Maintainable long-term | Still supported in 5 years |
| **Scalability** | Handles growth | From GB to TB of data |
| **Monitoring** | Track data quality | Automated testing, alerts |
| **Security** | Protects sensitive data | Encryption, access control |

---

### Best Practices

#### 1. Automated Testing and Proactive Alerts

**Example checks:**
```sql
-- Check data counts match after transformation
SELECT COUNT(*) FROM source_sales;  -- Result: 10,000

SELECT COUNT(*) FROM unified_sales; -- Should be: 10,000

-- Check totals remain consistent
SELECT SUM(amount) FROM source_sales;  -- Result: $1,250,000

SELECT SUM(amount) FROM unified_sales; -- Should be: $1,250,000
```

**Alert if:**
- Row counts don't match
- Total amounts differ
- Required fields are NULL
- Data types are incorrect

---

#### 2. Security and Access Control

**Example: Credit Card Anonymization**

**Original Data (Restricted):**
| customer_id | credit_card_number | purchase_amount |
|-------------|-------------------|-----------------|
| 1           | 4532-1234-5678-9012 | 150.00 |
| 2           | 5411-8765-4321-0987 | 200.00 |

**Transformed Data (Business Analysts):**
| customer_id | card_type | purchase_amount |
|-------------|-----------|-----------------|
| 1           | Visa      | 150.00 |
| 2           | Mastercard| 200.00 |

**Transformation:**
```python
# Identify card type from first 4 digits
first_four = credit_card_number[:4]

if first_four.startswith('4'):
    card_type = 'Visa'
elif first_four.startswith('5'):
    card_type = 'Mastercard'

# Don't include full number in unified model
```

---

#### 3. Data Governance - Lineage

**Lineage = Tracking data's journey**

**Example:**
```
Source: sales_db.transactions
    ‚Üì (extracted 2024-10-29 02:00)
Staging: staging.raw_transactions
    ‚Üì (transformed 2024-10-29 02:15)
Unified: warehouse.sales
    ‚Üì (used by)
Dashboard: daily_sales_report
ML Model: customer_churn_prediction
```

**Why important:**
- ‚úÖ Auditing compliance
- ‚úÖ Debugging data issues
- ‚úÖ Impact analysis (if source changes)
- ‚úÖ Data quality tracking

---

## Part 4: Database Management Systems (DBMS)

### What is a DBMS?

**Definition:** System software for creating and maintaining databases

**DBMS manages three aspects:**
1. **Data:** The actual information stored
2. **Database Schema:** Logical structure
3. **Database Engine:** Allows data access, locking, modification

**Role:** Interface between database and users/applications

```
Users/Applications
        ‚Üï
    [DBMS]
        ‚Üï
   Database
```

---

### Two Main Types

## 1. SQL DBMS (Relational DBMS)

### Characteristics
- Based on relational data model
- Uses SQL for queries
- Structured data with predefined schema
- Tables with rows and columns
- ACID compliance (Atomicity, Consistency, Isolation, Durability)

### Popular SQL DBMS Options

| DBMS | Best For | Key Features |
|------|----------|--------------|
| **PostgreSQL** | General purpose, complex queries | Open source, extensible, JSON support |
| **MySQL** | Web applications | Fast reads, popular with WordPress |
| **Microsoft SQL Server** | Enterprise applications | Windows integration, .NET support |
| **Oracle Database** | Large enterprises | High performance, enterprise features |
| **SQLite** | Mobile apps, embedded systems | Lightweight, serverless |

### Example: PostgreSQL Table
```sql
CREATE TABLE employees (
    employee_id SERIAL PRIMARY KEY,
    first_name VARCHAR(50) NOT NULL,
    last_name VARCHAR(50) NOT NULL,
    email VARCHAR(100) UNIQUE,
    hire_date DATE,
    salary DECIMAL(10, 2),
    department_id INT REFERENCES departments(id)
);

SELECT * FROM employees WHERE salary > 50000;
```

### ‚úÖ Use SQL DBMS When:

| Scenario | Reason |
|----------|--------|
| **Fixed structure** | Schema doesn't change frequently |
| **Data consistency critical** | Banking, accounting, ERP systems |
| **Complex relationships** | Many foreign keys and joins |
| **ACID transactions needed** | Financial transactions, inventory |
| **Structured data** | Well-defined rows and columns |

---

## 2. NoSQL DBMS (Non-Relational DBMS)

### Characteristics
- Document-centered (not table-centered)
- No predefined schema required
- Flexible data structures
- Horizontal scaling
- Eventually consistent (often)

### üîë Key-Value Store

**Structure:** Stores key-value pairs (like a dictionary)

**Data model:**
```
Key          ‚Üí Value
"user:1001"  ‚Üí "{'name': 'Alice', 'age': 30}"
"session:abc"‚Üí "{'user_id': 1001, 'cart': [...]}"
"cache:home" ‚Üí "<!DOCTYPE html><html>..."
```

**Popular DBMS:** Redis, Amazon DynamoDB, Riak

**Example (Redis):**
```redis
SET user:1001 "{'name': 'Alice', 'age': 30}"
GET user:1001
DEL user:1001
```

**‚úÖ Use for:**
- Session management (shopping carts)
- Caching (web pages, API responses)
- Real-time analytics
- Message queues

---

### üìÑ Document Store

**Structure:** Keys with structured document values (JSON, XML, BSON)

**Data model:**
```json
{
  "_id": "post_001",
  "title": "Introduction to NoSQL",
  "author": "Alice Johnson",
  "content": "NoSQL databases are...",
  "tags": ["database", "nosql", "tutorial"],
  "comments": [
    {
      "user": "Bob",
      "text": "Great post!",
      "date": "2024-10-15"
    }
  ],
  "created_at": "2024-10-01"
}
```

**Popular DBMS:** MongoDB, CouchDB, Amazon DocumentDB

**Example (MongoDB):**
```javascript
// Insert document
db.posts.insertOne({
  title: "Introduction to NoSQL",
  author: "Alice Johnson",
  tags: ["database", "nosql"],
  created_at: new Date()
});

// Query documents
db.posts.find({ tags: "database" });

// Update document
db.posts.updateOne(
  { _id: "post_001" },
  { $set: { author: "Alice Smith" } }
);
```

**‚úÖ Use for:**
- Content management systems (blogs, CMS)
- User profiles
- Product catalogs
- Mobile app backends

**Example Use Case: Blog Platform**
```
Each blog post = One document
{
  post_id, title, content, author, 
  comments[], tags[], metadata
}
```

---

### üóÇÔ∏è Columnar Database

**Structure:** Stores each column in separate files

**Traditional row-based:**
```
Row 1: [id=1, name=Alice, age=30, city=NYC]
Row 2: [id=2, name=Bob, age=25, city=LA]
Row 3: [id=3, name=Carol, age=35, city=NYC]
```

**Columnar:**
```
Column 'id': [1, 2, 3]
Column 'name': [Alice, Bob, Carol]
Column 'age': [30, 25, 35]
Column 'city': [NYC, LA, NYC]
```

**Popular DBMS:** Apache Cassandra, HBase, Amazon Redshift

**Example (Cassandra):**
```sql
CREATE TABLE user_events (
    user_id uuid,
    event_time timestamp,
    event_type text,
    value int,
    PRIMARY KEY (user_id, event_time)
);

-- Query billions of rows efficiently
SELECT event_type, SUM(value) 
FROM user_events 
WHERE event_time > '2024-01-01'
GROUP BY event_type;
```

**‚úÖ Use for:**
- Big data analytics
- Data warehousing
- Time-series data
- When you query specific columns (not all columns)

**Why it's faster:**
```
Query: SELECT SUM(sales) FROM transactions;

Row-based: Reads ALL columns for EVERY row
Columnar: Reads ONLY the 'sales' column
```

---

### üï∏Ô∏è Graph Database

**Structure:** Nodes (entities) and edges (relationships)

**Data model:**
```
(Alice)-[:FRIENDS_WITH]->(Bob)
(Bob)-[:WORKS_AT]->(Company A)
(Alice)-[:LIKES]->(Product X)
(Bob)-[:BOUGHT]->(Product X)
(Product X)-[:CATEGORY]->(Electronics)
```

**Popular DBMS:** Neo4j, Amazon Neptune, ArangoDB

**Example (Neo4j - Cypher query language):**
```cypher
// Create nodes and relationships
CREATE (alice:Person {name: 'Alice', age: 30})
CREATE (bob:Person {name: 'Bob', age: 25})
CREATE (alice)-[:FRIENDS_WITH]->(bob)

// Find friends of friends
MATCH (person:Person {name: 'Alice'})-[:FRIENDS_WITH]->()-[:FRIENDS_WITH]->(fof)
RETURN fof.name

// Recommendation: Products liked by friends
MATCH (me:Person {name: 'Alice'})-[:FRIENDS_WITH]->(friend)-[:LIKES]->(product)
WHERE NOT (me)-[:LIKES]->(product)
RETURN product.name, COUNT(*) as friend_likes
ORDER BY friend_likes DESC
```

**‚úÖ Use for:**
- Social networks (Facebook, LinkedIn)
- Recommendation engines (Netflix, Amazon)
- Fraud detection (transaction patterns)
- Knowledge graphs
- Network analysis

**Example Use Case: Social Network**
```
Nodes: Users, Posts, Groups
Edges: follows, likes, member_of, commented_on

Query: "Find all users who liked posts by people I follow"
```

---

## Choosing the Right DBMS

### Decision Tree

```
Do you have structured data with fixed schema?
    ‚îÇ
    ‚îú‚îÄYES‚îÄ‚îÄ> Do you need ACID transactions?
    ‚îÇ            ‚îÇ
    ‚îÇ            ‚îú‚îÄYES‚îÄ‚îÄ> Use SQL DBMS (PostgreSQL, MySQL)
    ‚îÇ            ‚îÇ
    ‚îÇ            ‚îî‚îÄNO‚îÄ‚îÄ‚îÄ> Consider your query patterns
    ‚îÇ                        ‚îÇ
    ‚îÇ                        ‚îî‚îÄ‚îÄ> Mostly analytical? ‚Üí Columnar (Redshift)
    ‚îÇ
    ‚îî‚îÄNO‚îÄ‚îÄ> What's your data structure?
                ‚îÇ
                ‚îú‚îÄSimple key-value pairs? ‚Üí Key-Value Store (Redis)
                ‚îÇ
                ‚îú‚îÄFlexible documents/JSON? ‚Üí Document Store (MongoDB)
                ‚îÇ
                ‚îú‚îÄHighly connected data? ‚Üí Graph Database (Neo4j)
                ‚îÇ
                ‚îî‚îÄBig data analytics? ‚Üí Columnar (Cassandra)
```

---

### Comparison Table

| Requirement | SQL DBMS | NoSQL DBMS |
|-------------|----------|------------|
| **Fixed schema** | ‚úÖ Required | ‚ùå Optional |
| **Scalability** | Vertical (bigger server) | Horizontal (more servers) |
| **Consistency** | Strong (ACID) | Eventual (BASE) |
| **Relationships** | Complex joins | Denormalized/embedded |
| **Query language** | SQL (standardized) | Varies by type |
| **Best for** | Structured, consistent data | Rapidly changing data |
| **Examples** | Banking, ERP, CRM | Social media, IoT, analytics |

---

### Real-World Examples

#### E-Commerce Platform

**SQL (PostgreSQL):**
- Orders, payments (need ACID)
- Customer accounts
- Inventory management

**NoSQL (MongoDB):**
- Product catalog (varying attributes)
- User reviews
- Shopping cart sessions

**NoSQL (Redis):**
- Session data
- Cache for popular products
- Real-time inventory counts

**NoSQL (Neo4j):**
- Product recommendations
- "Customers who bought this also bought..."

---

## Summary

### Database Roles
- **User roles:** Individual accounts
- **Group roles:** Collections of privileges
- Use GRANT/REVOKE for access control
- Roles are efficient and persistent

### Partitioning
- **Vertical:** Split by columns
- **Horizontal:** Split by rows (typically date)
- **Sharding:** Partition across servers
- Improves performance for large tables

### Data Integration
- Combines multiple sources into unified view
- Requires transformations (ETL)
- Consider: formats, update cadence, security
- Use tools like Airflow for orchestration

### DBMS Selection
- **SQL:** Structured data, ACID transactions, complex relationships
- **NoSQL:** Flexible schema, rapid growth, horizontal scaling
- Choose based on data structure and business needs
- Can use multiple DBMS types in same application (polyglot persistence)

---

## Quick Reference Cheat Sheet

### Role Management Commands

```sql
-- Create role
CREATE ROLE role_name [WITH options];

-- Modify role
ALTER ROLE role_name WITH attribute;

-- Delete role
DROP ROLE role_name;

-- Grant privileges
GRANT privilege ON object TO role;

-- Revoke privileges
REVOKE privilege ON object FROM role;

-- Add user to group
GRANT group_role TO user_role;

-- Remove user from group
REVOKE group_role FROM user_role;
```

---

### Partitioning Commands (PostgreSQL)

```sql
-- Create partitioned table
CREATE TABLE table_name (columns...)
PARTITION BY RANGE (column_name);

-- Create partition
CREATE TABLE partition_name
PARTITION OF table_name
FOR VALUES FROM (start) TO (end);

-- Create index
CREATE INDEX ON table_name (partition_column);
```

---

### DBMS Selection Quick Guide

| Your Situation | Recommended DBMS |
|----------------|------------------|
| Banking/financial system | PostgreSQL, Oracle |
| E-commerce product catalog | MongoDB |
| Real-time caching | Redis |
| Social network | Neo4j |
| Data warehouse/analytics | Redshift, Cassandra |
| Mobile app backend | MongoDB, Firebase |
| IoT sensor data | Cassandra, InfluxDB |
| Content management system | MongoDB, PostgreSQL |

---

## Practice Scenarios

### Scenario 1: New Data Team

**Situation:** You're setting up a data team with 5 analysts and 2 engineers.

**Solution:**
```sql
-- Create group roles
CREATE ROLE data_analyst;
CREATE ROLE data_engineer;

-- Grant appropriate privileges
GRANT SELECT ON ALL TABLES IN SCHEMA public TO data_analyst;
GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO data_engineer;

-- Create user roles
CREATE ROLE alice WITH LOGIN PASSWORD 'pw1';
CREATE ROLE bob WITH LOGIN PASSWORD 'pw2';
CREATE ROLE charlie WITH LOGIN PASSWORD 'pw3';

-- Assign to groups
GRANT data_analyst TO alice;
GRANT data_analyst TO bob;
GRANT data_engineer TO charlie;
```

---

### Scenario 2: Large Sales Table

**Situation:** Sales table with 500GB of data, queries are slow.

**Solution - Horizontal Partitioning:**
```sql
-- Create partitioned table
CREATE TABLE sales (
    sale_id BIGINT,
    sale_date DATE,
    amount DECIMAL,
    customer_id INT
) PARTITION BY RANGE (sale_date);

-- Create yearly partitions
CREATE TABLE sales_2022 PARTITION OF sales
FOR VALUES FROM ('2022-01-01') TO ('2023-01-01');

CREATE TABLE sales_2023 PARTITION OF sales
FOR VALUES FROM ('2023-01-01') TO ('2024-01-01');

CREATE TABLE sales_2024 PARTITION OF sales
FOR VALUES FROM ('2024-01-01') TO ('2025-01-01');

-- Queries now only scan relevant partitions
SELECT SUM(amount) FROM sales 
WHERE sale_date BETWEEN '2024-01-01' AND '2024-12-31';
-- Only scans sales_2024 partition!
```

---

### Scenario 3: Multi-Source Integration

**Situation:** Combine customer data from CRM (PostgreSQL) and website analytics (MongoDB).

**Planning:**

| Aspect | Decision |
|--------|----------|
| **Goal** | 360-degree customer view |
| **Sources** | CRM (PostgreSQL), Analytics (MongoDB) |
| **Target** | Data warehouse (Snowflake) |
| **Update** | Daily at 2 AM |
| **Tool** | Apache Airflow |

**Implementation:**
```python
from airflow import DAG
from datetime import datetime

dag = DAG(
    'customer_integration',
    schedule_interval='0 2 * * *',  # 2 AM daily
    start_date=datetime(2024, 1, 1)
)

# Define tasks
extract_crm = ExtractPostgreSQLTask(...)
extract_analytics = ExtractMongoDBTask(...)
transform_data = TransformTask(...)
load_warehouse = LoadSnowflakeTask(...)
validate_data = ValidateTask(...)

# Set dependencies
[extract_crm, extract_analytics] >> transform_data >> load_warehouse >> validate_data
```

---

### Scenario 4: Choosing DBMS for New Project

**Project:** Building a social media application

**Requirements Analysis:**

| Feature | Best DBMS | Reason |
|---------|-----------|--------|
| User profiles | MongoDB | Flexible schema (users have different fields) |
| Friend connections | Neo4j | Graph relationships |
| Session data | Redis | Fast key-value storage |
| Post history | PostgreSQL | Need transactions for consistency |
| Real-time feed | Cassandra | High write throughput |

**Architecture:**
```
User Registration ‚Üí PostgreSQL (ACID for accounts)
    ‚Üì
Profile Data ‚Üí MongoDB (flexible attributes)
    ‚Üì
Friendships ‚Üí Neo4j (graph queries)
    ‚Üì
Active Sessions ‚Üí Redis (fast access)
    ‚Üì
Post Feed ‚Üí Cassandra (scalable writes)
```

**Polyglot Persistence:** Using multiple database types for different needs!

---

## Common Mistakes to Avoid

### ‚ùå Role Management

| Mistake | Why It's Bad | Solution |
|---------|--------------|----------|
| Not using groups | Repetitive privilege assignments | Create group roles for common access levels |
| Overly permissive roles | Security risk | Grant minimum necessary privileges |
| Forgetting to revoke | Former employees retain access | Regular access audits |
| No documentation | Can't track who has what access | Document all roles and their purposes |

---

### ‚ùå Partitioning

| Mistake | Why It's Bad | Solution |
|---------|--------------|----------|
| Partitioning small tables | Adds complexity without benefit | Only partition tables >100GB |
| Wrong partition key | Queries still scan all partitions | Choose key used in WHERE clauses |
| Too many partitions | Management overhead | Balance between size and quantity |
| Not maintaining | Old partitions accumulate | Regular cleanup/archival strategy |

---

### ‚ùå Data Integration

| Mistake | Why It's Bad | Solution |
|---------|--------------|----------|
| No data validation | Corrupt data propagates | Automated testing at each step |
| Ignoring security | Sensitive data exposed | Anonymize/encrypt during ETL |
| No monitoring | Failures go unnoticed | Proactive alerts and logging |
| Manual processes | Not scalable or reliable | Use ETL tools and automation |

---

### ‚ùå DBMS Selection

| Mistake | Why It's Bad | Solution |
|---------|--------------|----------|
| Using only SQL | Missing benefits of NoSQL | Evaluate needs objectively |
| Using only NoSQL | Losing ACID when needed | Use SQL for transactional data |
| Following trends | Tool doesn't fit needs | Choose based on requirements |
| Not considering scale | Can't handle growth | Plan for 10x data volume |

---

## Key Takeaways

### üéØ Database Roles
1. Roles are the foundation of access control
2. Group roles save time and improve consistency
3. Use GRANT/REVOKE for fine-grained permissions
4. Audit roles regularly for security

### üéØ Partitioning
1. Partition when tables exceed 100GB
2. Horizontal partitioning splits by rows (common: by date)
3. Vertical partitioning splits by columns (rare)
4. Sharding distributes partitions across servers
5. Both OLAP and OLTP benefit from partitioning

### üéØ Data Integration
1. Plan: goal, sources, formats, update cadence
2. Transform data to unified format (ETL)
3. Use tools (Airflow, Talend) not manual code
4. Implement validation, monitoring, and security
5. Track data lineage for governance

### üéØ DBMS Selection
1. SQL for structured data and ACID transactions
2. NoSQL for flexibility and horizontal scaling
3. Four NoSQL types: Key-Value, Document, Columnar, Graph
4. Choose based on data structure and use case
5. Polyglot persistence: use multiple DBMS types together

---

## Final Exam Practice Questions

### Question 1: Role Management
**Scenario:** You need to create a role for junior analysts who should only read from the `sales` and `customers` tables, but not modify them.

**Your task:** Write the SQL commands.

<details>
<summary>Click to see answer</summary>

```sql
-- Create the role
CREATE ROLE junior_analyst;

-- Grant SELECT privilege
GRANT SELECT ON sales TO junior_analyst;
GRANT SELECT ON customers TO junior_analyst;

-- Create a user and assign to group
CREATE ROLE jane WITH LOGIN PASSWORD 'secure_pw';
GRANT junior_analyst TO jane;
```
</details>

---

### Question 2: Partitioning Decision
**Scenario:** You have a 500GB `orders` table. Queries mostly filter by `order_date`. Should you partition? If yes, how?

<details>
<summary>Click to see answer</summary>

**Yes, partition by date:**
- Table is large (>100GB)
- Queries filter by `order_date`
- Partition by month or quarter

```sql
CREATE TABLE orders (
    order_id BIGINT,
    order_date DATE,
    customer_id INT,
    total DECIMAL
) PARTITION BY RANGE (order_date);

CREATE TABLE orders_2024_q1 PARTITION OF orders
FOR VALUES FROM ('2024-01-01') TO ('2024-04-01');
-- Create additional partitions for other quarters
```
</details>

---

### Question 3: DBMS Selection
**Scenario:** You're building:
1. A banking system (transactions, accounts)
2. A product recommendation engine
3. A real-time chat application cache

**Which DBMS for each?**

<details>
<summary>Click to see answer</summary>

1. **Banking:** PostgreSQL or Oracle (SQL)
   - Need ACID transactions
   - Structured data
   - Data consistency critical

2. **Recommendation engine:** Neo4j (Graph)
   - Relationships between users and products
   - "Users who liked X also liked Y"
   - Graph queries efficient

3. **Chat cache:** Redis (Key-Value)
   - Real-time access
   - Simple key-value pairs
   - Fast read/write
</details>

---

### Question 4: Data Integration
**Scenario:** Integrating customer data from:
- Sales DB (PostgreSQL) - daily updates needed
- Support tickets (MongoDB) - hourly updates needed
- Credit card data (must anonymize last 12 digits)

**What's your approach?**

<details>
<summary>Click to see answer</summary>

**Approach:**
1. **Different update schedules:**
   - Sales: Extract daily at 2 AM
   - Support: Extract hourly

2. **Security transformation:**
```python
def anonymize_credit_card(card_number):
    # Keep first 4 digits, mask rest
    return card_number[:4] + '-XXXX-XXXX-XXXX'
```

3. **ETL Pipeline (Airflow):**
```python
# Daily sales DAG
extract_sales >> transform_sales >> load_warehouse

# Hourly support DAG  
extract_support >> transform_support >> load_warehouse

# Add validation
load_warehouse >> validate_totals >> alert_on_failure
```

4. **Validation:**
   - Check record counts match
   - Verify no credit card numbers in output
   - Alert on failures
</details>

---

## Conclusion

Database management encompasses many critical areas:

- **Access Control:** Use roles to manage permissions efficiently
- **Performance:** Partition large tables for better query performance  
- **Integration:** Combine disparate sources into unified views
- **Technology:** Choose the right DBMS for your data and use case

The key is understanding your requirements and applying the right technique or technology to solve your specific problem. There's rarely a one-size-fits-all solution in database management!
